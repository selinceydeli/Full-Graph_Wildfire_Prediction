{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:37.932558Z",
     "start_time": "2025-10-13T14:37:37.851670Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import rasterio\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import fiona\n",
    "import pandas as pd \n",
    "from shapely.strtree import STRtree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc67a2",
   "metadata": {},
   "source": [
    "## Part I: Creating our Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a25b9a6",
   "metadata": {},
   "source": [
    "#### File-Paths\n",
    "Understanding shapefiles: [Link](https://www.geowgs84.com/post/understanding-shapefiles-a-deep-dive-into-shp-dbf-shx-and-prj)\n",
    "\n",
    "- `.shp`: points, lines and polygons; encoded in binary, ebgin with a fixed-length header and each feature then has its own header and content.\n",
    "- `.shx`: spatial indexing - acts as an index for the `.shp` file - speeds up queries. Has fixed length records pointing to corresponding feature locations     \n",
    "- `.prj`: projection file - contains the coordinate system and projection of our shapes\n",
    "- `.dbf`: databased file - contains attribute data per shape - columns like name, population, elevation and the like.\n",
    "\n",
    "Alternatives we may see: \n",
    "- GeoJSON \n",
    "- GPKG - Geopackage\n",
    "\n",
    "*GeoPandas can be used for all vector files. Rasterio for all raster images*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e36c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:37.966379Z",
     "start_time": "2025-10-13T14:37:37.951826Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"data\"\n",
    "# Effis Fire Dataset - European Forest Fire Information System\n",
    "# https://forest-fire.emergency.copernicus.eu/applications/data-and-services\n",
    "FIRE_DATA_PATH = os.path.join(DATA_DIRECTORY, \"fire_data/modis.ba.poly.shp\")\n",
    "\n",
    "# Several TIFFs with Wildfire Severity, once again from Effis\n",
    "WILDFIRE_SEVERITY_DIR = os.path.join(DATA_DIRECTORY, \"wildfire_severity\")\n",
    "\n",
    "# Natural Earth - public domain dataset of global geography\n",
    "# Admin 0 – Countries dataset has country boundaries\n",
    "# https://www.naturalearthdata.com/downloads/110m-cultural-vectors/\n",
    "COUNTRY_BOUNDARY_PATH = os.path.join(DATA_DIRECTORY, \"country_boundaries/ne_110m_admin_0_countries.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62787bd33b594db5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:38.313684Z",
     "start_time": "2025-10-13T14:37:38.307650Z"
    }
   },
   "outputs": [],
   "source": [
    "coord_ref_sys = \"EPSG:25829\" \n",
    "# Coordinate reference system - defines how spatial data coordinates relate to the earth. Defines origin, units, orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9be871",
   "metadata": {},
   "source": [
    "### Filter out to our target area: Portugal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b26b68c95ea08f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:51.749831Z",
     "start_time": "2025-10-13T14:37:38.449144Z"
    }
   },
   "outputs": [],
   "source": [
    "# GeoPandas looks in the same directory for data files by default (.dbf, .shx, etc.)\n",
    "fire_data = gpd.read_file(FIRE_DATA_PATH)\n",
    "fire_data = fire_data.to_crs(coord_ref_sys) # reproject to our fixed CRS\n",
    "fire_data = fire_data[fire_data[\"COUNTRY\"] == \"PT\"]\n",
    "# fire_data = fire_data.iloc[:100]\n",
    "fire_data.head()\n",
    "fire_data[\"FIREDATE\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8480d2c16cdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:51.870467Z",
     "start_time": "2025-10-13T14:37:51.846463Z"
    }
   },
   "outputs": [],
   "source": [
    "fire_data.head()\n",
    "# fire_data[\"FIREDATE\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63b0b08d2ba6d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:52.073570Z",
     "start_time": "2025-10-13T14:37:51.995627Z"
    }
   },
   "outputs": [],
   "source": [
    "world = gpd.read_file(COUNTRY_BOUNDARY_PATH)\n",
    "portugal = world[world[\"NAME\"] == \"Portugal\"]\n",
    "\n",
    "# https://epsg.io/25829\n",
    "# Reprojection of Portugal to our fixed CRS (specified above)\n",
    "portugal_proj = portugal.to_crs(coord_ref_sys) # degrees -> met\n",
    "mainland = portugal_proj.iloc[portugal_proj.geometry.area.argmax()]\n",
    "mainland_polygon = mainland.geometry\n",
    "\n",
    "# Extract bordering box around Portugal\n",
    "minx, miny, maxx, maxy = mainland_polygon.bounds\n",
    "print(minx, miny, maxx, maxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b311f71f52f0119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:52.430791Z",
     "start_time": "2025-10-13T14:37:52.421957Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_size = 10000 # meters - discretization size of what one node is in our graph\n",
    "x_coords = np.arange(minx, maxx, grid_size)\n",
    "y_coords = np.arange(miny, maxy, grid_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9bf51ddc4ed394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:53.286700Z",
     "start_time": "2025-10-13T14:37:52.454196Z"
    }
   },
   "outputs": [],
   "source": [
    "# create grid\n",
    "grid_squares = []\n",
    "for x in x_coords:\n",
    "    for y in y_coords:\n",
    "        # this makes a full rectangle grid\n",
    "        cell = box(x, y, x + grid_size, y + grid_size)\n",
    "        # only add cells that are inside mainland portugal (leave out the sea)\n",
    "        if mainland_polygon.contains(cell):\n",
    "            grid_squares.append(cell)\n",
    "\n",
    "grid = gpd.GeoDataFrame({\"geometry\": grid_squares}, crs=coord_ref_sys)\n",
    "print(f\"Num nodes = {len(grid_squares)}\")\n",
    "grid.head()\n",
    "# each entry is lower-left, upper-left, upper-right, lower-right, lower-left points of each square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40bceaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:55.047601Z",
     "start_time": "2025-10-13T14:37:53.406268Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "grid.plot(ax=ax, edgecolor='black', facecolor='lightblue')\n",
    "ax.ticklabel_format(style='sci', scilimits=(4,4))\n",
    "ax.set_title(\"Portugal Grid - Shapefile\")\n",
    "ax.set_xlabel(\"Easting (m)\")\n",
    "ax.set_ylabel(\"Northing (m)\")\n",
    "plt.savefig(\"portugal_grid_plot.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759024036d4bb4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:55.850101Z",
     "start_time": "2025-10-13T14:37:55.105668Z"
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIRECTORY = 'output'\n",
    "os.makedirs(OUTPUT_DIRECTORY, exist_ok=True)\n",
    "grid.to_file(os.path.join(OUTPUT_DIRECTORY, \"portugal_grid.shp\"))\n",
    "grid.to_file(os.path.join(OUTPUT_DIRECTORY,\"portugal_grid.gpkg\"), layer=\"grid\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3beb589ce4f35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:55.920161Z",
     "start_time": "2025-10-13T14:37:55.867477Z"
    }
   },
   "outputs": [],
   "source": [
    "# add columns for coordinates of grid squares' centers\n",
    "grid_gdf = grid.copy()\n",
    "#rename geometry to geometry grid\n",
    "# grid_gdf = grid_gdf.rename(columns={'geometry': 'geometry_grid'})\n",
    "grid_gdf[\"centroid_grid\"] = grid.geometry.centroid\n",
    "grid_gdf[\"centroid_x_grid\"] = grid.geometry.centroid.x\n",
    "grid_gdf[\"centroid_y_grid\"] = grid.geometry.centroid.y\n",
    "grid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1acd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.317242Z",
     "start_time": "2025-10-13T14:37:56.053185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to GDB of Water\n",
    "gdb_path = os.path.join(DATA_DIRECTORY, \"rivers\", \"euhydro_tajo_v013.gdb\")\n",
    "\n",
    "# List of layer indices we need\n",
    "layer_indices = [0, 1, 6, 8, 9]\n",
    "\n",
    "# Read and concatenate all water layers\n",
    "water_gdfs = []\n",
    "for idx in layer_indices:\n",
    "    layer_name = fiona.listlayers(gdb_path)[idx]\n",
    "    gdf_layer = gpd.read_file(gdb_path, layer=layer_name)\n",
    "    water_gdfs.append(gdf_layer)\n",
    "\n",
    "# Combine all layers into a single GeoDataFrame\n",
    "water_gdf = gpd.GeoDataFrame(pd.concat(water_gdfs, ignore_index=True))\n",
    "print(\"Combined water bodies:\", water_gdf.shape)\n",
    "\n",
    "grid_gdf = grid_gdf.set_geometry('centroid_grid')\n",
    "\n",
    "# Ensure the CRS matches the water layer\n",
    "grid_gdf = grid_gdf.to_crs(water_gdf.crs)\n",
    "\n",
    "water_gdf = water_gdf[water_gdf.geometry.notnull()]\n",
    "water_geoms = water_gdf.geometry.values\n",
    "water_tree = STRtree(water_gdf.geometry.values)\n",
    "\n",
    "def nearest_water(point):\n",
    "    if point is None or point.is_empty:\n",
    "        return None\n",
    "    # nearest() now returns index\n",
    "    nearest_idx = water_tree.nearest(point)\n",
    "    nearest_geom = water_geoms[nearest_idx]  # get the actual geometry\n",
    "    return point.distance(nearest_geom)\n",
    "\n",
    "grid_gdf['dist_to_water'] = grid_gdf.geometry.apply(nearest_water)\n",
    "grid_gdf.to_crs(coord_ref_sys, inplace=True)\n",
    "grid_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af0d73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.357631Z",
     "start_time": "2025-10-11T21:36:36.143434Z"
    }
   },
   "outputs": [],
   "source": [
    "grid_gdf = grid_gdf.reset_index(drop=True)\n",
    "grid_gdf[\"node_id\"] = np.arange(len(grid_gdf))\n",
    "grid_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264bb728",
   "metadata": {},
   "source": [
    "#### Overlay with Fire Data\n",
    "\n",
    "Assumption - if a centroid overlaps with a fire point - this is our fire node. We want to make this into a series, per date, for each fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b7e1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.401752600Z",
     "start_time": "2025-10-11T20:29:34.544013Z"
    }
   },
   "outputs": [],
   "source": [
    "# fires_with_centroids[\"centroid_x\"] = fire_data[\"GEOMETRY\"]\n",
    "fire_data[\"centroid_x_fire\"] = fire_data['geometry'].apply(lambda x: x.centroid.x)\n",
    "fire_data[\"centroid_y_fire\"] = fire_data['geometry'].apply(lambda y: y.centroid.y)\n",
    "fire_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7568e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.473336800Z",
     "start_time": "2025-10-11T20:44:12.960442Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "fire_data[\"DAY\"] = pd.to_datetime(fire_data[\"FIREDATE\"], format='mixed').dt.date\n",
    "# fire_data[\"FIREDATE\"].nunique()\n",
    "# fire_data[\"DAY\"].nunique()\n",
    "grouped_by_day = fire_data.groupby(\"DAY\").size().reset_index(name='fire_count')\n",
    "grouped_by_day.head()\n",
    "grouped_by_day.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e597a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.501405200Z",
     "start_time": "2025-10-11T20:44:19.615571Z"
    }
   },
   "outputs": [],
   "source": [
    " def safe_stat(value, func=\"mean\"):\n",
    "    \"\"\"Safely compute a statistic from a pandas Series, scalar, or missing value.\"\"\"\n",
    "    if value is None or isinstance(value, (float, int, np.generic, type(pd.NA))):\n",
    "        return value if isinstance(value, (float, int)) else None\n",
    "    \n",
    "    try:\n",
    "        if func == \"mean\":\n",
    "            return value.mean()\n",
    "        elif func == \"sum\":\n",
    "            return value.sum()\n",
    "        elif func == \"mode\":\n",
    "            mode = value.mode()\n",
    "            return mode.iloc[0] if not mode.empty else None\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7675c803eecfeac",
   "metadata": {},
   "source": [
    "Clustering in terms of weather data to reduce number of API calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e14697",
   "metadata": {},
   "source": [
    "### Aggregate Grid Based on Day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71ee3a5",
   "metadata": {},
   "source": [
    "# Important: This is currently set to break after one step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead948fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.505467200Z",
     "start_time": "2025-10-11T21:36:53.878760Z"
    }
   },
   "outputs": [],
   "source": [
    "from meteostat import Point, Hourly\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fire_data = fire_data.to_crs(grid_gdf.crs)\n",
    "graphs = []\n",
    "for day in tqdm(grouped_by_day[\"DAY\"]):\n",
    "    fire_data_day = fire_data[fire_data[\"DAY\"] == day].copy()\n",
    "    fire_data_day['geometry'] = fire_data_day.geometry.centroid\n",
    "\n",
    "    fire_centroids = fire_data_day.copy()\n",
    "    fire_centroids['geometry'] = fire_data_day.geometry.centroid\n",
    "    \n",
    "    # Perform spatial join: fire centroid within grid polygons - Faster than iterating over each centroid - https://www.youtube.com/watch?v=y85IKthrV-s\n",
    "    # We join POINT with POLYGON\n",
    "\n",
    "    grid_gdf = grid_gdf.set_geometry(\"geometry\")\n",
    "    fire_centroids = fire_centroids.set_geometry(\"geometry\")\n",
    "    joined = gpd.sjoin(grid_gdf, fire_centroids, how=\"left\", predicate=\"covers\")\n",
    "\n",
    "    agg = (\n",
    "        joined\n",
    "        .assign(_hit = joined[\"index_right\"].notna().astype(int))\n",
    "        .groupby(level=0)  # group by grid cell index\n",
    "        .agg(fire_count=(\"index_right\", \"count\"),\n",
    "             has_fire=(\"_hit\", \"max\"))\n",
    "    )\n",
    "\n",
    "    # Merge back to geometry; fill missing (no matches) with zeros\n",
    "    joined_one_row = grid_gdf.join(agg, how=\"left\").fillna({\"fire_count\": 0, \"has_fire\": 0})\n",
    "    joined_one_row[\"fire_count\"] = joined_one_row[\"fire_count\"].astype(int)\n",
    "    joined_one_row[\"has_fire\"]   = joined_one_row[\"has_fire\"].astype(int)\n",
    "\n",
    "    joined_one_row[\"node_id\"] = grid_gdf[\"node_id\"].values\n",
    "    joined_one_row[\"DAY\"] = pd.to_datetime(day).normalize()\n",
    "\n",
    "    graphs.append(joined_one_row)\n",
    "    # joined[\"geometry_centroid\"] = joined.geometry.centroid\n",
    "    # coords = [(geom.x, geom.y) for geom in joined.geometry_centroid]\n",
    "    #\n",
    "    # # Note: this method does not handle an edge case where multiple fires occured at the same place,\n",
    "    # joined[\"fire_intensity\"] = 0\n",
    "    #\n",
    "    # for filename in os.listdir(WILDFIRE_SEVERITY_DIR):\n",
    "    #     if filename.endswith(\".tiff\"):\n",
    "    #         filepath = os.path.join(WILDFIRE_SEVERITY_DIR, filename)\n",
    "    #         with rasterio.open(filepath) as src:\n",
    "    #\n",
    "    #             # Ensure CRS match\n",
    "    #             if joined.crs != src.crs:\n",
    "    #                 joined = joined.to_crs(src.crs)\n",
    "    #                 joined[\"geometry_centroid\"] = joined.geometry.centroid\n",
    "    #                 coords = [(geom.x, geom.y) for geom in joined.geometry_centroid]\n",
    "    #\n",
    "    #             values = np.array([val[0] for val in src.sample(coords)])\n",
    "    #             joined[\"fire_intensity\"] = np.maximum(joined[\"fire_intensity\"], values)\n",
    "    #\n",
    "    # print(joined.columns)\n",
    "    # results = []\n",
    "    # i = 0\n",
    "    # for point, date in tqdm(zip(joined[\"geometry_centroid\"], joined[\"DAY\"]), total=len(joined)):\n",
    "    #     if point is None or pd.isna(date):\n",
    "    #         continue\n",
    "    #\n",
    "    #     location = Point(point.y, point.x)\n",
    "    #\n",
    "    #     # UTC timestamps for Meteostat\n",
    "    #     start = pd.Timestamp(date)\n",
    "    #     end = start + timedelta(days=1)\n",
    "    #\n",
    "    #     # Fetch hourly data\n",
    "    #     df = Hourly(location, start, end).fetch()\n",
    "    #     if df.empty:\n",
    "    #         i+=1\n",
    "    #\n",
    "    #     if not df.empty:\n",
    "    #         # Aggregate all hours in that day\n",
    "    #         results.append({\n",
    "    #             \"DAY\": date,\n",
    "    #             \"lat\": point.y,\n",
    "    #             \"lon\": point.x,\n",
    "    #             \"temp_mean\": safe_stat(df[\"temp\"], \"mean\"),\n",
    "    #             \"rhum_mean\": safe_stat(df[\"rhum\"], \"mean\"),\n",
    "    #             \"wdir_mean\": safe_stat(df[\"wdir\"], \"mean\"),\n",
    "    #             \"wspd_mean\": safe_stat(df[\"wspd\"], \"mean\"),\n",
    "    #             \"pres_mean\": safe_stat(df[\"pres\"], \"mean\"),\n",
    "    #         })\n",
    "    #\n",
    "    # weather_df = pd.DataFrame(results)\n",
    "    # joined = pd.concat([joined, weather_df], axis=1)\n",
    "    #\n",
    "    # graphs.append(joined_one_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4755d5069d48641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.516117300Z",
     "start_time": "2025-10-11T21:37:32.547536Z"
    }
   },
   "outputs": [],
   "source": [
    "joined_one_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63effd9d3227a0cc",
   "metadata": {},
   "source": [
    "## Convert to .npy format with one feature (number of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9976d6b",
   "metadata": {},
   "source": [
    "#### Distance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8857ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cent = grid_gdf.geometry.centroid\n",
    "N = len(cent)\n",
    "A = np.zeros(shape = (N, N))\n",
    "\n",
    "grid_points = grid_gdf[\"geometry\"].to_numpy()\n",
    "P1s = np.array(list(map(lambda x: np.array([x.centroid.x, x.centroid.y]), grid_points[:] )))\n",
    "P2s = np.array(list(map(lambda x: np.array([x.centroid.x, x.centroid.y]), grid_points[:] )))\n",
    "\n",
    "A = np.sum((P1s[:,None,:] - P2s[None,:,:])**2, axis=2)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce49642",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/distance_matrix.npy\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de02f8df",
   "metadata": {},
   "source": [
    "#### Timeseries Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70939584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Timeseries Data\n",
    "# Concatenate all days into one long table (keep only what we need)\n",
    "long = pd.concat(\n",
    "    [\n",
    "        g[[\"node_id\", \"fire_count\", \"DAY\"]].copy()\n",
    "          .assign(DAY=pd.to_datetime(g[\"DAY\"]).dt.normalize())  # ensure consistent dtype\n",
    "        for g in graphs\n",
    "    ],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# If any duplicates slipped in per (node_id, DAY), sum them:\n",
    "long = (long\n",
    "        .groupby([\"node_id\", \"DAY\"], as_index=False)[\"fire_count\"]\n",
    "        .sum())\n",
    "\n",
    "# Create a node index and sorted day index:\n",
    "node_index = (grid_gdf[[\"node_id\"]]\n",
    "              .drop_duplicates()\n",
    "              .sort_values(\"node_id\")\n",
    "              .set_index(\"node_id\")\n",
    "              .index)\n",
    "\n",
    "day_index = (long[\"DAY\"]\n",
    "             .dropna()\n",
    "             .sort_values()\n",
    "             .unique())\n",
    "\n",
    "#Pivot to wide matrix (N x T), fill missing vals with 0:\n",
    "wide = (long\n",
    "        .pivot_table(index=\"node_id\", columns=\"DAY\", values=\"fire_count\", aggfunc=\"sum\", fill_value=0)\n",
    "        .reindex(index=node_index, columns=day_index, fill_value=0)\n",
    "        .sort_index(axis=0)\n",
    "        .sort_index(axis=1))\n",
    "\n",
    "#Final (N,T) float32 array:\n",
    "timeseries_data = wide.to_numpy(dtype=np.float32)  # shape (10041, 1284)\n",
    "\n",
    "np.save(\"data/timeseries_data.npy\", timeseries_data)\n",
    "np.save(\"data/days.npy\", day_index.astype(\"datetime64[ns]\"))\n",
    "# grid_gdf[[\"centroid_x_grid\", \"centroid_y_grid\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727788c556030d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-14T16:05:41.649223Z",
     "start_time": "2025-10-14T16:05:05.280676Z"
    }
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import save_npz\n",
    "# from sklearn.neighbors import radius_neighbors_graph\n",
    "# Construct Distance Matrix\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# print(A)\n",
    "    \n",
    "    \n",
    "# XY = np.c_[cent.x.values, cent.y.values]\n",
    "# N = XY.shape[0]\n",
    "\n",
    "# # Estimate cell size from centroid grid\n",
    "# ux = np.unique(np.round(XY[:,0], 6))\n",
    "# dx = np.median(np.diff(np.sort(ux)))\n",
    "# radius_m = 4 * dx\n",
    "\n",
    "\n",
    "# A = radius_neighbors_graph(\n",
    "#     XY, radius=radius_m, mode=\"connectivity\", include_self=False\n",
    "# ).tocsr().astype(np.uint8)\n",
    "\n",
    "# A = ((A + A.T) > 0).astype(np.uint8)\n",
    "\n",
    "# save_npz(\"data/adjacency_radius_4cells.npz\", A)\n",
    "# print(\"adj nnz:\", A.nnz, \"avg deg:\", A.nnz / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7721037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63000cff322b1d2b",
   "metadata": {},
   "source": [
    "## 2nd try with clustering for weather data\n",
    "To reduce the number of API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784bcb3df5c1c0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.519901400Z",
     "start_time": "2025-10-11T16:06:59.592940Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "grid_gdf.to_crs(coord_ref_sys)\n",
    "coords = np.array([(geom.y, geom.x) for geom in grid_gdf.centroid_grid])\n",
    "\n",
    "# clustering based on geographic closeness\n",
    "n_clusters = 100\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init = 10)\n",
    "grid_gdf[\"cluster_id\"] = kmeans.fit_predict(coords)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "grid_gdf.plot(column=\"cluster_id\", categorical=True, ax=ax, cmap=\"tab20\")\n",
    "grid_gdf.geometry.centroid.plot(ax=ax, color=\"black\", markersize=5, alpha=0.6)\n",
    "ax.set_title(\"KMeans Clusters of Grid Squares\")\n",
    "ax.set_axis_off()\n",
    "plt.savefig(\"kmeans_weather.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "grid_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efe76be822c5d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.519901400Z",
     "start_time": "2025-10-11T16:07:01.882779Z"
    }
   },
   "outputs": [],
   "source": [
    "from meteostat import Point, Hourly\n",
    "from datetime import timedelta\n",
    "\n",
    "start = pd.Timestamp(fire_data[\"DAY\"].min())\n",
    "end = pd.Timestamp(fire_data[\"DAY\"].max() + timedelta(days=1))\n",
    "\n",
    "cluster_weather = []\n",
    "\n",
    "for cluster_id, group in grid_gdf.groupby(\"cluster_id\"):\n",
    "    # Fetch weather data for each cluster (instead of every grid square)\n",
    "    lat = group.geometry.centroid.y.mean()\n",
    "    lon = group.geometry.centroid.x.mean()\n",
    "\n",
    "    # fetch weather data for the full date range\n",
    "    df_weather = Hourly(Point(lat, lon), start, end).fetch()\n",
    "\n",
    "    if not df_weather.empty:\n",
    "        daily = df_weather.resample(\"D\").mean().reset_index() # daily mean\n",
    "        daily[\"DAY\"] = daily[\"time\"].dt.date\n",
    "        daily[\"cluster_id\"] = cluster_id\n",
    "        # temperature, relative humidity, wind direction, wind speed, air pressure\n",
    "        cluster_weather.append(daily[[\"cluster_id\", \"DAY\", \"temp\", \"rhum\", \"wdir\", \"wspd\", \"pres\"]])\n",
    "\n",
    "weather_df = pd.concat(cluster_weather, ignore_index=True)\n",
    "weather_df.to_csv(\"cluster_weather.csv\", index=False)\n",
    "\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f1dee4",
   "metadata": {},
   "source": [
    "### Original Fire Attributes\n",
    "- **`id`** → Unique identifier for each fire record.  \n",
    "- **`FIREDATE`** → Date and time when the fire occurred.  \n",
    "- **`LASTUPDATE`** → Timestamp of the last update for the fire record.  \n",
    "- **`COUNTRY`** → Country where the fire occurred.  \n",
    "- **`PROVINCE`** → Province of the fire location.  \n",
    "- **`COMMUNE`** → Commune (local administrative area) of the fire.  \n",
    "- **`AREA_HA`** → Burned area in hectares.  \n",
    "\n",
    "### Land Cover / Fire Area Composition\n",
    "- **`BROADLEA`** → Proportion of broadleaf vegetation in the fire area.  \n",
    "- **`CONIFER`** → Proportion of coniferous vegetation.  \n",
    "- **`MIXED`** → Proportion of mixed forest.  \n",
    "- **`SCLEROPH`** → Proportion of sclerophyllous vegetation (e.g., Mediterranean shrubs).  \n",
    "- **`TRANSIT`** → Proportion of transitional land cover.  \n",
    "- **`OTHERNATLC`** → Proportion of other natural land cover.  \n",
    "- **`AGRIAREAS`** → Proportion of agricultural areas.  \n",
    "- **`ARTIFSURF`** → Proportion of artificial surfaces (urban/industrial).  \n",
    "- **`OTHERLC`** → Proportion of other land cover types.  \n",
    "- **`PERCNA2K`** → Percentage of area under special designation (e.g., Natura 2000).  \n",
    "- **`CLASS`** → Fire classification or severity category.  \n",
    "\n",
    "### Fire Geometry / Location\n",
    "- **`geometry`** → Original fire geometry (polygon or point).  \n",
    "- **`centroid_x_fire`** → X coordinate of fire centroid.  \n",
    "- **`centroid_y_fire`** → Y coordinate of fire centroid.  \n",
    "\n",
    "### Spatial Join with Grid Cells\n",
    "- **`index_right`** → Index of the grid cell containing the fire centroid (NaN if none).  \n",
    "- **`centroid_grid`** → Geometry of the grid cell centroid.  \n",
    "- **`centroid_x_grid`** → X coordinate of the grid cell centroid.  \n",
    "- **`centroid_y_grid`** → Y coordinate of the grid cell centroid.  \n",
    "- **`has_fire`** → Boolean indicating if the fire is inside a grid cell.  \n",
    "\n",
    "### Time-Based Helper\n",
    "- **`DAY`** → Date only (no time), extracted from `FIREDATE`, used for grouping fires by day.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884076a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.524384400Z",
     "start_time": "2025-10-11T16:07:04.677114Z"
    }
   },
   "outputs": [],
   "source": [
    "# A summary and sample of the graphs list - + persistence\n",
    "import pickle\n",
    "with open('output/graphs.pkl', 'wb') as f:\n",
    "    pickle.dump(graphs, f)\n",
    "    \n",
    "print(\"Number of time steps(graphs)\", len(graphs))\n",
    "print(\"Example graph for one day:\")\n",
    "graphs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f77f135c3a68d",
   "metadata": {},
   "source": [
    "## Attempt to Convert to .npy Format for the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba0a607d7c15fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T14:37:56.528935700Z",
     "start_time": "2025-10-11T16:07:04.867535Z"
    }
   },
   "outputs": [],
   "source": [
    "date_col = \"FIREDATE\"   # adjust to your real column name\n",
    "fire_data[date_col] = pd.to_datetime(fire_data[date_col]).dt.normalize()\n",
    "full_days = pd.date_range(\n",
    "    fire_data[date_col].min(),\n",
    "    fire_data[date_col].max(),\n",
    "    freq=\"D\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
